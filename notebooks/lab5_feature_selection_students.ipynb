{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab 5 - Feature selection\n",
    "\n",
    "In the previous lab we introduced the concept of _data dimensionality_, that is the number of variables, or _features_, used to represent our data. Think of features as... well, features, that is characteristics of the data.\n",
    "\n",
    "For instance, in a gene expression microarray dataset from a case-control association study, the input data $X$ will be a $n \\times d$ matrix where each _row_ is a vector of length $d$ which contains the expression levels of $d$ genes for a single individual.\n",
    "\n",
    "In some fields such as the analysis of biomolecular data $d$ may be in the order of the tens or hundreds of thousands, or even millions. The point is, they are not all equally important. On the contrary, it is safe to assume that _only a small subset_ of all the available features is actually _relevant_, that is plays a role in the relationship between input and output.\n",
    "\n",
    "For this reason, a crucial step is to _reduce the dimensionality of the data_ by identifying which are the relevant variables.\n",
    "\n",
    "Main concepts:\n",
    "\n",
    " * Data dimensionality\n",
    " * Feature selection\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Taxonomy of variable selection methods\n",
    "\n",
    "Roughly, feature selection methods are divided in three groups, based on how they select the features:\n",
    "\n",
    " * **Filter methods**: select subsets of variables as a pre-processing step, independently of the chosen learning machine. Example: statistical univariate tests (t-test, etc.).\n",
    " * **Wrapper methods**: Use the learning machine of interest as a black box to score subsets of variable according to their predictive power. Example: Recursive Feature Elimination (RFE).\n",
    " * **Embedded methods**: Perform variable selection _as part_ of the training process. Examples: Lasso, ElasticNET\n",
    " \n",
    "In this tutorial we will see different kinds of algorithms for feature selection, however, because of how they are used in the learning pipeline, they will _behave_ like filter methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Imports and setup\n",
    "\n",
    "The usual stuff:\n",
    "\n",
    " * Magic command `%matplotlib inline` so that plots are displayed correctly in the notebook.\n",
    " * `matplotlib` followed by `seaborn` in order to have fancy plots\n",
    " * `numpy` _et similia_ for number crunching.\n",
    " \n",
    "Additional libraries will be imported whenever they are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dataset generation\n",
    "\n",
    "We generate a synthetic classification dataset again using the `make_classification` function from the `scikit-learn` library. However, this time we choose higher values for $d$, and also set the number of _informative_ (i.e., relevant) variables to be much smaller than $d$.\n",
    "\n",
    "Notice that from now on we will no longer we able to plot the datasets, since the data dimensionality is too high.\n",
    "\n",
    "<img style=\"float: left;\" src=\"warning.png\" width=\"20px\"> &nbsp; **Warning**: by setting the argument `shuffle=False` we force the first `d_informative` columns of the data matrix to be those relative to relevant variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "### Set the number of samples and dimensions\n",
    "n, d = 100, 5000\n",
    "d_informative = 10\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "X, y = make_classification(n_samples=n, \n",
    "                           n_features=d, \n",
    "                           n_informative=d_informative, \n",
    "                           n_redundant=0, \n",
    "                           n_repeated=0, \n",
    "                           n_classes=2, \n",
    "                           n_clusters_per_class=1, \n",
    "                           flip_y=0.02, \n",
    "                           class_sep=1.0, \n",
    "                           shuffle=False,\n",
    "                           )\n",
    "\n",
    "### We have to shuffle the dataset manually, because reasons\n",
    "idx = np.arange(n)\n",
    "np.random.shuffle(idx)\n",
    "X = X[idx, :]\n",
    "y = y[idx]\n",
    "\n",
    "# X = X[:, :d_informative]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now split the dataset in half (first half for training, second half for test) and fit a Linear SVM (use `sklearn.svm.LinearSVC`) on the training set, using KCV to select the best model.\n",
    "Then, predict the labels for the test set and compute the prediction accuracy.\n",
    "\n",
    "<img style=\"float: left;\" src=\"info.png\" width=\"20px\"> &nbsp; **Hint**: I included a \"suggested\" range for the `C` parameter of the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.64\n"
     ]
    }
   ],
   "source": [
    "C_range = np.logspace(-4,1, 20)\n",
    "\n",
    "### BEGIN STUDENTS ###\n",
    "\n",
    "### END STUDENTS ###\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy score: {}\".format(acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now reduce the dataset to the relevant variables only (they are the first `d_informative` columns of the $X$. Slice the matrix using the `:` operator as seen before).\n",
    "\n",
    "Of course we can do this only with a synthetic dataset where we know in advance _which_ are the relevant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "C_range = np.logspace(-4,1, 20)\n",
    "\n",
    "### BEGIN STUDENTS ###\n",
    "\n",
    "### END STUDENTS ###\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy score: {}\".format(acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You should notice that the accuracy has greatly improved, as the noise introduced by those extra, useless variables is no longer in the dataset.\n",
    "\n",
    "However, in real case scenarios we do not know in advance which variables to keep and which ones to throw away.\n",
    "\n",
    "### Enter variable selection!\n",
    "\n",
    "Now repeat the experiment above, this time adding a variable selection step to the mix.\n",
    "\n",
    "Use Recursive Feature Elimination from (RFE, docs [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)) with a Linear SVM classifier (the same one as before) as estimator.\n",
    "\n",
    "<img style=\"float: left;\" src=\"info.png\" width=\"20px\"> &nbsp; **Hint**: Use the suggested step `0.1` otherwise the feature selection process will take too long.\n",
    "\n",
    "<img style=\"float: left;\" src=\"warning.png\" width=\"20px\"> &nbsp; **Waring**: Due to how the RFE is implemented, parameters to the grid search object must be passed in a slightly different way, here is an example:\n",
    "\n",
    "```python\n",
    "...\n",
    "grid_search = GridSearchCV(\n",
    "                        rfe_est, \n",
    "                        param_grid={\n",
    "                            'n_features_to_select':[5, 10, 20], \n",
    "                            'estimator__C':C_range # <- look here!\n",
    "                        }, \n",
    "                        cv=4)\n",
    "...\n",
    "```\n",
    "\n",
    "Notice how the values for the `C` parameter of the inner estimator has to be passed by prepending `estimator__` to the key in the dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "### BEGIN STUDENTS ###\n",
    "\n",
    "\n",
    "### END STUDENTS ###\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy score: {}\".format(acc_score))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
